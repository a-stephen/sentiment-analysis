{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b785350-2fe5-42c7-ab8c-86253ae49877",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to C:\\Users\\asus tuf\n",
      "[nltk_data]     gaming\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\asus tuf\n",
      "[nltk_data]     gaming\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\asus tuf\n",
      "[nltk_data]     gaming\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package sentiwordnet to C:\\Users\\asus tuf\n",
      "[nltk_data]     gaming\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package sentiwordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "pip3_path = '.local/lib/python3.5/site-packages'\n",
    "if pip3_path not in sys.path:\n",
    "    sys.path.append(pip3_path)\n",
    "\n",
    "# general\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import requests\n",
    "import os\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import tarfile\n",
    "from zipfile import ZipFile\n",
    "from time import time\n",
    "from pprint import pprint\n",
    "from glob import glob\n",
    "\n",
    "# sklearn\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# nltk\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import sentiwordnet as swn, wordnet as wn\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('sentiwordnet')\n",
    "\n",
    "# other\n",
    "from afinn import Afinn\n",
    "from textblob.en.sentiments import PatternAnalyzer as TextBlobPatternAnalyzer\n",
    "from textblob.en.sentiments import NaiveBayesAnalyzer as TextBlobNaiveBayesAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ca95e11-1b48-4a27-8644-c7ea8c421789",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Long docs already downloaded.\n",
      "Extracting long docs...\n",
      "Reading long docs...\n",
      "Downloading short docs...\n",
      "Short docs downloaded.\n",
      "Downloads complete.\n",
      "Long pos 25000\n",
      "Long neg 25000\n",
      "Short pos 1500\n",
      "Short neg 1500\n"
     ]
    }
   ],
   "source": [
    "def load_long_docs(max_docs_per_label=np.inf):\n",
    "\n",
    "    if not os.path.exists('aclImdb_v1.tar.gz'):\n",
    "       \n",
    "        print('Downloading long docs...')\n",
    "        response = requests.get('http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz')\n",
    "\n",
    "        with open('aclImdb_v1.tar.gz', 'wb') as f:  \n",
    "            f.write(response.content)\n",
    "\n",
    "    else:\n",
    "        print('Long docs already downloaded.')\n",
    "        \n",
    "    if not os.path.exists('long_docs'):\n",
    "       \n",
    "        print('Extracting long docs...')\n",
    "        with tarfile.open('aclImdb_v1.tar.gz') as tar:\n",
    "            tar.extractall('long_docs')\n",
    "            tar.close()\n",
    "\n",
    "    else:\n",
    "        print('Long docs already extracted.')\n",
    "\n",
    "\n",
    "    print('Reading long docs...')\n",
    "\n",
    "    pos_sentences, neg_sentences = [], []\n",
    "\n",
    "    count = 0\n",
    "    for path in glob('long_docs/aclImdb/*/neg/*.txt'):\n",
    "        with open(path, 'r', errors = \"replace\") as f:\n",
    "            text = f.read()\n",
    "            neg_sentences.append(text)\n",
    "            count += 1\n",
    "            if count >= max_docs_per_label:\n",
    "                break\n",
    "\n",
    "    count = 0\n",
    "    for path in glob('long_docs/aclImdb/*/pos/*.txt'):\n",
    "        with open(path, 'r', errors = \"replace\") as f:\n",
    "            text = f.read()\n",
    "            pos_sentences.append(text)\n",
    "            count += 1\n",
    "            if count >= max_docs_per_label:\n",
    "                break\n",
    "\n",
    "    return {'pos': pos_sentences, 'neg': neg_sentences}\n",
    "\n",
    "def load_short_docs():\n",
    "\n",
    "    if not os.path.exists('short_docs'):\n",
    "       \n",
    "        print('Downloading short docs...')\n",
    "        response = requests.get('http://archive.ics.uci.edu/ml/machine-learning-databases/00331/sentiment labelled sentences.zip')\n",
    "\n",
    "        with open('short_docs.zip', 'wb') as f:  \n",
    "            f.write(response.content)\n",
    "           \n",
    "        ZipFile('short_docs.zip').extractall('short_docs')\n",
    "        print('Short docs downloaded.')\n",
    "   \n",
    "    else:\n",
    "\n",
    "        print('Short docs already downloaded.')\n",
    "   \n",
    "    pos_sentences, neg_sentences = [], []\n",
    "\n",
    "    # pandas.read_csv failed to process one of the files correctly, hence this manual approach.\n",
    "    for path in glob('short_docs/sentiment labelled sentences/*labelled.txt'):\n",
    "        with open(path, 'r') as f:\n",
    "            pairs = [line.split('\\t') for line in f.readlines()]\n",
    "            for pair in pairs:\n",
    "                if int(pair[1]):\n",
    "                    pos_sentences.append(pair[0])\n",
    "                else:\n",
    "                    neg_sentences.append(pair[0])\n",
    "\n",
    "    return {'pos': pos_sentences, 'neg': neg_sentences}\n",
    "\n",
    "def print_doc_counts(doc_sets):\n",
    "   \n",
    "    for key1 in doc_sets:\n",
    "        for key2 in doc_sets[key1]:\n",
    "            print(key1, key2, len(doc_sets[key1][key2]))\n",
    "            \n",
    "# Next, we'll call our functions to load the long and short corpora. If you have your own corpus readily available, you can add it to the full_doc_sets collection below, and it will automatically be included in the tests that follow.\n",
    "\n",
    "# The long corpus is quite large and takes a long time to train models on. You can choose to load fewer than the 50,000 documents by specifying the optional parameter max_docs_per_label, but you'll also have an option later in the script to train on only a subset of whatever you have downloaded.\n",
    "\n",
    "full_doc_sets = {'Long': load_long_docs(), # (max_docs_per_label=5000) # optionally, limit size for faster loading\n",
    "                 'Short': load_short_docs(),\n",
    "               # 'Your corpus here': {'pos': [...your positive docs...], 'neg': [...your negative docs...]}\n",
    "                }\n",
    "\n",
    "doc_set_names = full_doc_sets.keys()\n",
    "\n",
    "print('Downloads complete.')\n",
    "print_doc_counts(full_doc_sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "842b93e0-6751-4fae-8dc1-86f866ec3253",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Long docs already downloaded.\n",
      "Long docs already extracted.\n",
      "Reading long docs...\n",
      "Short docs already downloaded.\n",
      "Downloads complete.\n",
      "Long pos 25000\n",
      "Long neg 25000\n",
      "Short pos 1500\n",
      "Short neg 1500\n"
     ]
    }
   ],
   "source": [
    "full_doc_sets = {'Long': load_long_docs(), # (max_docs_per_label=5000) # optionally, limit size for faster loading\n",
    "                 'Short': load_short_docs(),\n",
    "               # 'Your corpus here': {'pos': [...your positive docs...], 'neg': [...your negative docs...]}\n",
    "                }\n",
    "\n",
    "doc_set_names = full_doc_sets.keys()\n",
    "\n",
    "print('Downloads complete.')\n",
    "print_doc_counts(full_doc_sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "20aae798-1d3f-49d8-9c64-f25fd60cd122",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:\n",
      "Long pos 20000\n",
      "Long neg 20000\n",
      "Short pos 1200\n",
      "Short neg 1200\n",
      "Test: \n",
      "Long pos 5000\n",
      "Long neg 5000\n",
      "Short pos 300\n",
      "Short neg 300\n"
     ]
    }
   ],
   "source": [
    "global_results = {}\n",
    "\n",
    "train_doc_sets = {key: {} for key in full_doc_sets}\n",
    "test_doc_sets = {key: {} for key in full_doc_sets}\n",
    "\n",
    "\n",
    "for name in doc_set_names:\n",
    "    for label in ['pos', 'neg']:\n",
    "        train, test = train_test_split(full_doc_sets[name][label], test_size = 0.2)\n",
    "        train_doc_sets[name][label], test_doc_sets[name][label] = train, test\n",
    "        \n",
    "print('Train:')\n",
    "print_doc_counts(train_doc_sets)\n",
    "print('Test: ')\n",
    "print_doc_counts(test_doc_sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60c3c12-222e-4a1d-8088-067997d2fc39",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
